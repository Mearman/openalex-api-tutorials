{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\" style=\"border:solid 1px gray;\">\n",
    "    <a href=\"https://openalex.org/\">\n",
    "        <img src=\"../../resources/img/OpenAlex-banner.png\" alt=\"OpenAlex banner\" width=\"300\">\n",
    "    </a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How are my institution's researchers collaborating with people around the globe?\n",
    "\n",
    "In this tutorial, we will use OpenAlex's API to get all of the publications that are collaborations between a given institution and other institutions around the world.\n",
    "\n",
    "We will use the University of Washington (UW) as our example institution. This is a **real-world example:** The UW Office of Global Affairs has built an [interactive publications dashboard](https://www.washington.edu/global/publications/) which lets people dig deep into these collaborations using maps, graphs, and other visualizations. This lets them show off the scholarly collaborations between UW's faculty and researchers around the world.\n",
    "\n",
    "The data powering this dashboard is based on the Microsoft Academic Graph, which is no longer being updated. So we will use OpenAlex's API to get freshly updated data for UW.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's look at how the University of Washington is represented in OpenAlex.\n",
    "\n",
    "[Institutions](https://docs.openalex.org/api-entities/institutions) in OpenAlex are closely linked with the [ROR registry](https://ror.org/) of research organizations. By searching the ROR website, we can find the [ROR ID for UW](https://ror.org/00cvxb145)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "uw_id = \"https://ror.org/00cvxb145\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the data for UW publications\n",
    "\n",
    "To get the data for the UW dashboard, we need to collect all of the works from OpenAlex which have at least one author from UW and at least one author outside UW. We'll do this in two steps: first, we'll get all of the works with UW authors, then we'll filter to keep only the papers with at least one other affiliation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure to put your email address in the `email` variable. This is how you use the [polite pool](https://docs.openalex.org/how-to-use-the-api/rate-limits-and-authentication#the-polite-pool), which will get you faster and more consistent response times when using the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete URL with filters:\n",
      "https://api.openalex.org/works?filter=institutions.ror:https://ror.org/00cvxb145,from_publication_date:2003-01-01\n"
     ]
    }
   ],
   "source": [
    "### ADD YOUR EMAIL to use the polite pool\n",
    "email = \"\"\n",
    "\n",
    "# specify endpoint\n",
    "endpoint = 'works'\n",
    "\n",
    "# build the 'filter' parameter\n",
    "# We'll limit it to the last 20 years\n",
    "filters = \",\".join((\n",
    "    f'institutions.ror:{uw_id}',\n",
    "    'from_publication_date:2003-01-01',\n",
    "))\n",
    "\n",
    "# put the URL together\n",
    "filtered_works_url = f'https://api.openalex.org/{endpoint}?filter={filters}'\n",
    "if email:\n",
    "    filtered_works_url += f\"&mailto={email}\"\n",
    "print(f'complete URL with filters:\\n{filtered_works_url}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've built the URL. We are requesting a list of Works from the API, using two [filters](https://docs.openalex.org/api-entities/works/filter-works) to specify which works we want. If you like, [you can use your browser](https://docs.openalex.org/how-to-use-the-api/rate-limits-and-authentication#calling-the-api-in-your-browser) (such as Firefox or Chrome) to go to the URL we've built and view the data. (Make sure you have a browser extension which formats JSON in a nice way. Otherwise the data will look like a big mess on your screen.)\n",
    "\n",
    "We're more interested in getting the data into Python. So let's get the results from the API, using the `requests` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retrieved 25 works\n"
     ]
    }
   ],
   "source": [
    "r = requests.get(filtered_works_url)\n",
    "results_page = r.json()\n",
    "print(f\"retrieved {len(results_page['results'])} works\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've retrieved 25 works, but of course that isn't the total number of works by UW authors. We only got the first page of results. We can see how many works there actually are by looking at the `results_page['meta']['count']` value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256646\n"
     ]
    }
   ],
   "source": [
    "print(results_page['meta']['count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10265.84"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# there are 25 results per page\n",
    "results_page['meta']['count'] / 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the full publications data\n",
    "\n",
    "There are about 250,000 works. To get all of them, we will need to [get multiple pages of results](https://docs.openalex.org/how-to-use-the-api/get-lists-of-entities/paging). Since we want more than 10,000 works, we need to use the cursor paging technique. At 25 results per page, that means we will need to make more than\n",
    "10,000 API calls to get all of the data we want.\n",
    "\n",
    "This may seem like a lot, but don't panic! This is well under the [free allowance of 100,000 API calls per day.](https://docs.openalex.org/how-to-use-the-api/rate-limits-and-authentication) It will take about an hour in total, so let it run overnight or while you take a break.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 api requests made so far\n",
      "10 api requests made so far\n",
      "20 api requests made so far\n",
      "50 api requests made so far\n",
      "100 api requests made so far\n",
      "500 api requests made so far\n",
      "1000 api requests made so far\n",
      "1500 api requests made so far\n",
      "2000 api requests made so far\n",
      "2500 api requests made so far\n",
      "3000 api requests made so far\n",
      "3500 api requests made so far\n",
      "4000 api requests made so far\n",
      "4500 api requests made so far\n",
      "5000 api requests made so far\n",
      "5500 api requests made so far\n",
      "6000 api requests made so far\n",
      "6500 api requests made so far\n",
      "7000 api requests made so far\n",
      "7500 api requests made so far\n",
      "8000 api requests made so far\n",
      "8500 api requests made so far\n",
      "9000 api requests made so far\n",
      "9500 api requests made so far\n",
      "10000 api requests made so far\n",
      "done. made 10265 api requests. collected 256584 works\n"
     ]
    }
   ],
   "source": [
    "cursor = '*'\n",
    "\n",
    "select = \",\".join((\n",
    "    'id',\n",
    "    'ids',\n",
    "    'title',\n",
    "    'display_name',\n",
    "    'publication_year',\n",
    "    'publication_date',\n",
    "    'primary_location',\n",
    "    'open_access',\n",
    "    'authorships',\n",
    "    'cited_by_count',\n",
    "    'is_retracted',\n",
    "    'is_paratext',\n",
    "    'updated_date',\n",
    "    'created_date',\n",
    "))\n",
    "\n",
    "# loop through pages\n",
    "works = []\n",
    "loop_index = 0\n",
    "while cursor:\n",
    "    \n",
    "    # set cursor value and request page from OpenAlex\n",
    "    url = f'{filtered_works_url}&select={select}&cursor={cursor}'\n",
    "    page_with_results = requests.get(url).json()\n",
    "    \n",
    "    results = page_with_results['results']\n",
    "    works.extend(results)\n",
    "\n",
    "    # update cursor to meta.next_cursor\n",
    "    cursor = page_with_results['meta']['next_cursor']\n",
    "    loop_index += 1\n",
    "    if loop_index in [5, 10, 20, 50, 100] or loop_index % 500 == 0:\n",
    "        print(f'{loop_index} api requests made so far')\n",
    "print(f'done. made {loop_index} api requests. collected {len(works)} works')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the raw data for later (optional)\n",
    "\n",
    "Now might be a good time to save the data we just retrieved, in case this notebook restarts and we want to come back to it without having to get all of the works from the API again. The following cell allows us to do this by saving the data using the [`.pickle`](https://docs.python.org/3/library/pickle.html) data format. (If you are loading the data in this way, then you don't need to run the above cell.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# # uncomment these lines and run to save the results so we won't have to fetch them\n",
    "# # again next time we run the notebook\n",
    "# import os\n",
    "# if not os.path.isdir('../../data'):\n",
    "#     os.mkdir('../../data')\n",
    "# with open('../../data/uw_works_since_2003.pickle', 'wb') as outf:\n",
    "#     pickle.dump(works, outf, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# # OR uncomment these lines and run to load the saved results\n",
    "with open('../../data/uw_works_since_2003.pickle', 'rb') as f:\n",
    "    works = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Pandas to organize the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each work object has a lot of information, not all of which we will use. Let's get the data into a [Pandas dataframe](https://pandas.pydata.org/), limiting the number of fields to just the ones we might be interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = []\n",
    "for work in works:\n",
    "    for authorship in work['authorships']:\n",
    "        if authorship:\n",
    "            author = authorship['author']\n",
    "            author_id = author['id'] if author else None\n",
    "            author_name = author['display_name'] if author else None\n",
    "            author_position = authorship['author_position']\n",
    "            for institution in authorship['institutions']:\n",
    "                if institution:\n",
    "                    institution_id = institution['id']\n",
    "                    institution_name = institution['display_name']\n",
    "                    institution_country_code = institution['country_code']\n",
    "                    data.append({\n",
    "                        'work_id': work['id'],\n",
    "                        'work_title': work['title'],\n",
    "                        'work_display_name': work['display_name'],\n",
    "                        'work_publication_year': work['publication_year'],\n",
    "                        'work_publication_date': work['publication_date'],\n",
    "                        'author_id': author_id,\n",
    "                        'author_name': author_name,\n",
    "                        'author_position': author_position,\n",
    "                        'institution_id': institution_id,\n",
    "                        'institution_name': institution_name,\n",
    "                        'institution_country_code': institution_country_code,\n",
    "                    })\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter for outside collaborations\n",
    "\n",
    "We are only interested in publications which have at least one authorship outside of UW, so let's label each row with this information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outside_uw_collab(institution_ids):\n",
    "    # Function that takes institution IDs (grouped by works)\n",
    "    # and returns True if the work has at least one non-UW affiliation\n",
    "    if all(institution_ids == 'https://openalex.org/I201448701'):\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "# transform() will return a series the same length as the dataframe, \n",
    "# which we can store as a column in the dataframe\n",
    "df['is_outside_uw_collab'] = df.groupby('work_id')['institution_id'].transform(outside_uw_collab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create a new dataframe with only the True rows (works that are collaborations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataframe has 1,657,788 rows, with 161,883 unique publications\n"
     ]
    }
   ],
   "source": [
    "df_collab = df[df['is_outside_uw_collab']].drop(columns='is_outside_uw_collab')\n",
    "print(f\"dataframe has {len(df_collab):,} rows, with {df_collab['work_id'].nunique():,} unique publications\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may also want to know how many of these collaborations are with institutions *outside the US,* rather than just outside UW. We can use a similar technique to answer this question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 97,275 publications with international collaboration.\n"
     ]
    }
   ],
   "source": [
    "def international_collab(institution_country_codes):\n",
    "    # Function that takes institution country codes (grouped by works)\n",
    "    # and returns True if the work has at least one non-US affiliated institution\n",
    "    if all(institution_country_codes == 'US'):\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "df_collab['is_international_collab'] = df_collab.groupby('work_id')['institution_country_code'].transform(international_collab)\n",
    "num_international_collab = df_collab[df_collab['is_international_collab']]['work_id'].nunique()\n",
    "print(f\"There are {num_international_collab:,} publications with international collaboration.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the data for Institutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 22,983 institutions that collaborate with UW, in 200 countries.\n"
     ]
    }
   ],
   "source": [
    "print(f\"There are {df_collab['institution_id'].nunique():,} institutions that collaborate with UW, in {df_collab['institution_country_code'].nunique()} countries.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query the API for institutions data\n",
    "\n",
    "We've found about 20K institutions that collaborate with UW. We already know the names and the countries of these institutions; these were included in the results from the `works` endpoint of the API. There is some more information about the institutions that would be helpful to supply to the dashboard, especially the geolocation data that can be used to show the collaborations on maps. This information can be found in the [Institutions](https://docs.openalex.org/api-entities/institutions) entities.\n",
    "\n",
    "To get the institutions, we will query the `/institutions` endpoint of the API. We will use the technique to get multiple entities per API request as suggested [here](https://docs.openalex.org/how-to-use-the-api/get-lists-of-entities/filter-entity-lists#addition-or). This technique involves getting 50 results at once by requesting them in a filter, separated by a pipe ('`|`').\n",
    "\n",
    "This will get all of the institutions in about 400 API calls, which should only take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "institution_ids = df['institution_id'].dropna().unique()\n",
    "\n",
    "endpoint = \"institutions\"\n",
    "size = 50\n",
    "loop_index = 0\n",
    "institutions = []\n",
    "for list_index in range(0, len(institution_ids), size):\n",
    "    subset = institution_ids[list_index:list_index+size]\n",
    "    pipe_separated_ids = \"|\".join(subset)\n",
    "    r = requests.get(f\"https://api.openalex.org/institutions?filter=openalex:{pipe_separated_ids}&per-page={size}\")\n",
    "    results = r.json()['results']\n",
    "    institutions.extend(results)\n",
    "    loop_index += 1\n",
    "print(f\"collected {len(institutions)} institutions using {loop_index} api calls\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, let's put this data into a Pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for institution in institutions:\n",
    "    data.append({\n",
    "        'id': institution['id'],\n",
    "        'ror': institution['ror'],\n",
    "        'display_name': institution['display_name'],\n",
    "        'country_code': institution['country_code'],\n",
    "        'type': institution['type'],\n",
    "        'latitude': institution['geo']['latitude'],\n",
    "        'longitude': institution['geo']['longitude'],\n",
    "        'city': institution['geo']['city'],\n",
    "        'region': institution['geo']['region'],\n",
    "        'country': institution['geo']['country'],\n",
    "        'image_url': institution['image_url'],\n",
    "        'image_thumbnail_url': institution['image_thumbnail_url'],\n",
    "    })\n",
    "\n",
    "df_institutions = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the data\n",
    "\n",
    "Now it's time to save the data, so it can be handed off to the team at UW and be imported into their dashboard. We'll save two different files:\n",
    "- The publications data will have one row for each publication-author-affiliation triplet. Each publication will have multiple rows, because there are multiple authors per paper. An author in a given publication could have more than one row, if they have more than one affiliation.\n",
    "- The institutions data will have one row per institution, connected to the publications data by the `institution_id` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.isdir('../../data'):\n",
    "    os.mkdir('../../data')\n",
    "    \n",
    "# Save the publications data\n",
    "# Each row represents a publication-author-affiliation\n",
    "#\n",
    "# Compressing the data as a GZIP file will greatly reduce the file size.\n",
    "# Pandas knows to use GZIP compression if the file name ends in `.gz`\n",
    "outpath = '../../data/uw_collabs.csv.gz'\n",
    "df_collab.to_csv(outpath, index=False)\n",
    "\n",
    "# Save the institutions data\n",
    "outpath = '../../data/uw_collabs_institutions.csv.gz'\n",
    "df_institutions.to_csv(outpath, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the data\n",
    "\n",
    "While the UW's own dashboard does a great job of visualizing the data in sophisticated ways, it's also fun to whip up a few quick visualizations of our own. We can start with simply showing the number of international collaborations among UW publications over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collaborations over time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the python visualization library [Plotly](https://plotly.com/python/)—specifically, the [`plotly.express`](https://plotly.com/python/plotly-express/) module. It is very easy to get up and running visualizing our data, and as a bonus, the plots are interactive. (Interactive charts will not work if you're viewing this pre-rendered on github. They will only work if you are running the notebook yourself. See inline comments.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "# import plotly.io as pio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the international collaborations by year\n",
    "international_collabs_by_year = df_collab.groupby('work_publication_year')['is_international_collab'].sum().reset_index(name='count')\n",
    "\n",
    "# Exclude the year 2023\n",
    "international_collabs_by_year = international_collabs_by_year[international_collabs_by_year['work_publication_year']<2023]\n",
    "\n",
    "fig = px.line(international_collabs_by_year, x='work_publication_year', y='count')\n",
    "# for an interactive plot, remove the renderer parameter, and just run `fig.show()`\n",
    "fig.show(renderer=\"svg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's plot the same data, but this time by country. We'll plot the top ten countries.\n",
    "\n",
    "We'll use the [country_converter](https://github.com/IndEcol/country_converter) python library to get more readable names for the countries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import country_converter as coco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the counts by year by country\n",
    "country_by_year = df_collab.groupby(['work_publication_year', 'institution_country_code']).size().reset_index(name='count')\n",
    "\n",
    "# Get the names of the countries from the country codes\n",
    "cc = coco.CountryConverter()\n",
    "country_by_year['country'] = cc.pandas_convert(country_by_year['institution_country_code'], to='name_short')\n",
    "\n",
    "# Filter out collaborations with other US institutions\n",
    "country_by_year = country_by_year[country_by_year['institution_country_code']!='US']\n",
    "\n",
    "# Exclude the year 2023\n",
    "country_by_year = country_by_year[country_by_year['work_publication_year']<2023]\n",
    "\n",
    "# Get the top ten countries\n",
    "top_countries = country_by_year.groupby('institution_country_code')['count'].sum().sort_values(ascending=False).head(10).index\n",
    "country_by_year_subset = country_by_year[country_by_year['institution_country_code'].isin(top_countries)]\n",
    "\n",
    "# Plot the data\n",
    "fig = px.line(country_by_year_subset, x='work_publication_year', y='count', color='country')\n",
    "# for an interactive plot, remove the renderer parameter, and just run `fig.show()`\n",
    "fig.show(renderer='svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chloropleth map\n",
    "\n",
    "Finally, let's make a [chloropleth map](https://en.wikipedia.org/wiki/Choropleth_map), which uses color to show the number of collaborations in countries around the world."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_counts = df_collab['institution_country_code'].value_counts()\n",
    "country_counts = country_counts[country_counts.index!='US']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_counts = country_counts.rename_axis('country_code').reset_index(name='num_collaborations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the names of the countries from the country codes\n",
    "cc = coco.CountryConverter()\n",
    "country_counts['country'] = cc.pandas_convert(country_counts['country_code'], to='name_short')\n",
    "# The Plotly chloropleth method needs the ISO3 (3-letter) country code\n",
    "country_counts['country_code_ISO3'] = cc.pandas_convert(country_counts['country_code'], to='ISO3')\n",
    "\n",
    "# Make the map\n",
    "fig = px.choropleth(country_counts, locations='country_code_ISO3', locationmode='ISO-3', color='num_collaborations', hover_name='country')\n",
    "# for an interactive plot, remove the renderer parameter, and just run `fig.show()`\n",
    "fig.show(renderer='svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "271691dbc4cdb85f541c883090ff5a004cbd8b9c207c2cfed84437fce4e65fdb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

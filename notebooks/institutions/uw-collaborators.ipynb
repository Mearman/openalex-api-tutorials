{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How are University of Washington (UW) researchers collaborating with people around the globe?\n",
    "\n",
    "The UW Office of Global Affairs is interested in showing off the scholarly collaborations between UW's faculty and researchers around the world. Their [interactive dashboard](https://www.washington.edu/global/publications/) lets people dig deep into these collaborations using maps, graphs, and other visualizations. The data powering this is dashboard is based on the Microsoft Academic Graph, which is no longer being updated. In this tutorial, we will use OpenAlex's API to update the data by getting all of the publications that are collaborations between UW faculty and other institutions.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's look at how the University of Washington is represented in OpenAlex.\n",
    "\n",
    "[Institutions](https://docs.openalex.org/api-entities/institutions) in OpenAlex are closely linked with the [ROR registry](https://ror.org/) of research organizations. By searching the ROR website, we can find the [ROR ID for UW](https://ror.org/00cvxb145)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "uw_id = \"https://ror.org/00cvxb145\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the data for UW publications\n",
    "\n",
    "To get the data for the UW dashboard, we need to collect all of the works from OpenAlex which have at least one author from UW and at least one author outside UW. We'll do this in two steps: first, we'll get all of the works with UW authors, then we'll filter to keep only the papers with at least one other affiliation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete URL with filters:\n",
      "https://api.openalex.org/works?filter=institutions.ror:https://ror.org/00cvxb145,from_publication_date:2003-01-01\n"
     ]
    }
   ],
   "source": [
    "# specify endpoint\n",
    "endpoint = 'works'\n",
    "\n",
    "# build the 'filter' parameter\n",
    "# We'll limit it to the last 20 years\n",
    "filters = \",\".join((\n",
    "    f'institutions.ror:{uw_id}',\n",
    "    'from_publication_date:2003-01-01',\n",
    "))\n",
    "\n",
    "# put the URL together\n",
    "filtered_works_url = f'https://api.openalex.org/{endpoint}?filter={filters}'\n",
    "print(f'complete URL with filters:\\n{filtered_works_url}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've built the URL. Now let's get the results from the API, using the `requests` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retrieved 25 works\n"
     ]
    }
   ],
   "source": [
    "r = requests.get(filtered_works_url)\n",
    "results_page = r.json()\n",
    "print(f\"retrieved {len(results_page['results'])} works\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've retrieved 25 works, but of course that isn't the total number of works by UW authors. We only got the first page of results. We can see how many works there actually are by looking at the `results_page['meta']['count']` value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "246644\n"
     ]
    }
   ],
   "source": [
    "print(results_page['meta']['count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9865.76"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_page['meta']['count'] / 25"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the full publications data\n",
    "\n",
    "There are about 250,000 works. To get all of them, we will need to use the [paging technique](https://docs.openalex.org/how-to-use-the-api/get-lists-of-entities/paging). Since we want more than 10,000 works, we need to use the cursor paging technique. At 25 results per page, that means we will need to make nearly 10,000 API calls to get all of the data we want. This may seem like a lot, but don't panic! This is well under the [free allowance of 100,000 API calls per day.](https://docs.openalex.org/how-to-use-the-api/rate-limits-and-authentication) It will take about an hour in total, so let it run overnight or while you take a break.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 api requests made so far\n",
      "10 api requests made so far\n",
      "20 api requests made so far\n",
      "50 api requests made so far\n",
      "100 api requests made so far\n",
      "500 api requests made so far\n",
      "1000 api requests made so far\n",
      "1500 api requests made so far\n",
      "2000 api requests made so far\n",
      "2500 api requests made so far\n",
      "3000 api requests made so far\n",
      "3500 api requests made so far\n",
      "4000 api requests made so far\n",
      "4500 api requests made so far\n",
      "5000 api requests made so far\n",
      "5500 api requests made so far\n",
      "6000 api requests made so far\n",
      "6500 api requests made so far\n",
      "7000 api requests made so far\n",
      "7500 api requests made so far\n",
      "8000 api requests made so far\n",
      "8500 api requests made so far\n",
      "9000 api requests made so far\n",
      "9500 api requests made so far\n",
      "done. made 9867 api requests. collected 246640 works\n"
     ]
    }
   ],
   "source": [
    "cursor = '*'\n",
    "\n",
    "select = \",\".join((\n",
    "    'id',\n",
    "    'ids',\n",
    "    'title',\n",
    "    'display_name',\n",
    "    'publication_year',\n",
    "    'publication_date',\n",
    "    'primary_location',\n",
    "    'open_access',\n",
    "    'authorships',\n",
    "    'cited_by_count',\n",
    "    'is_retracted',\n",
    "    'is_paratext',\n",
    "    'updated_date',\n",
    "    'created_date',\n",
    "))\n",
    "\n",
    "# loop through pages\n",
    "works = []\n",
    "loop_index = 0\n",
    "while cursor:\n",
    "    \n",
    "    # set cursor value and request page from OpenAlex\n",
    "    url = f'{filtered_works_url}&select={select}&cursor={cursor}'\n",
    "    page_with_results = requests.get(url).json()\n",
    "    \n",
    "    results = page_with_results['results']\n",
    "    works.extend(results)\n",
    "\n",
    "    # update cursor to meta.next_cursor\n",
    "    cursor = page_with_results['meta']['next_cursor']\n",
    "    loop_index += 1\n",
    "    if loop_index in [5, 10, 20, 50, 100] or loop_index % 500 == 0:\n",
    "        print(f'{loop_index} api requests made so far')\n",
    "print(f'done. made {loop_index} api requests. collected {len(works)} works')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the raw data for later\n",
    "\n",
    "Now might be a good time to save the data we just retrieved, in case this notebook restarts and we want to come back to it without having to get all of the works from the API again. The following cell allows us to do this by saving the data using the `.pickle` data format. (If you are loading the data in this way, then you don't need to run the above cell.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# uncomment these lines and run to save the results so we won't have to fetch them\n",
    "# again next time we run the notebook\n",
    "# import os\n",
    "# if not os.path.isdir('../../data'):\n",
    "#     os.mkdir('../../data')\n",
    "# with open('../../uw_works_since_2003.pickle', 'wb') as outf:\n",
    "#     pickle.dump(works, outf, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# OR uncomment these lines and run to load the saved results\n",
    "# with open('../../data/uw_works_since_2003.pickle', 'rb') as f:\n",
    "#     works = pickle.load(f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each work object has a lot of information, not all of which we will use. Let's get the data into a Pandas dataframe, limiting the number of fields to just the ones we might be interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = []\n",
    "for work in works:\n",
    "    for authorship in work['authorships']:\n",
    "        if authorship:\n",
    "            author = authorship['author']\n",
    "            author_id = author['id'] if author else None\n",
    "            author_name = author['display_name'] if author else None\n",
    "            author_position = authorship['author_position']\n",
    "            for institution in authorship['institutions']:\n",
    "                if institution:\n",
    "                    institution_id = institution['id']\n",
    "                    institution_name = institution['display_name']\n",
    "                    institution_country_code = institution['country_code']\n",
    "                    data.append({\n",
    "                        'work_id': work['id'],\n",
    "                        'work_title': work['title'],\n",
    "                        'work_display_name': work['display_name'],\n",
    "                        'work_publication_year': work['publication_year'],\n",
    "                        'work_publication_date': work['publication_date'],\n",
    "                        'author_id': author_id,\n",
    "                        'author_name': author_name,\n",
    "                        'author_position': author_position,\n",
    "                        'institution_id': institution_id,\n",
    "                        'institution_name': institution_name,\n",
    "                        'institution_country_code': institution_country_code,\n",
    "                    })\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter for outside collaborations\n",
    "\n",
    "We are only interested in publications which have at least one authorship outside of UW, so let's label each row with this information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outside_uw_collab(institution_ids):\n",
    "    # Function that takes institution IDs (grouped by works)\n",
    "    # and returns True if the work has at least one non-UW affiliation\n",
    "    if all(institution_ids == 'https://openalex.org/I201448701'):\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "# transform() will return a series the same length as the dataframe, \n",
    "# which we can store as a column in the dataframe\n",
    "df['is_outside_uw_collab'] = df.groupby('work_id')['institution_id'].transform(outside_uw_collab)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create a new dataframe with only the True rows (works that are collaborations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataframe has 1,454,811 rows, with 149,736 unique publications\n"
     ]
    }
   ],
   "source": [
    "df_collab = df[df['is_outside_uw_collab']].drop(columns='is_outside_uw_collab')\n",
    "print(f\"dataframe has {len(df_collab):,} rows, with {df_collab['work_id'].nunique():,} unique publications\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a side note, we may be curious about how many of these collaborations are with institutions *outside the US,* rather than just outside UW. We can use a similar technique to answer this question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 94,565 publications with international collaboration.\n"
     ]
    }
   ],
   "source": [
    "def international_collab(institution_country_codes):\n",
    "    # Function that takes institution country codes (grouped by works)\n",
    "    # and returns True if the work has at least one non-US affiliated institution\n",
    "    if all(institution_country_codes == 'US'):\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "df_collab['is_international_collab'] = df_collab.groupby('work_id')['institution_country_code'].transform(international_collab)\n",
    "num_international_collab = df_collab[df_collab['is_international_collab']]['work_id'].nunique()\n",
    "print(f\"There are {num_international_collab:,} publications with international collaboration.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the data for Institutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 20,852 institutions that collaborate with UW, in 193 countries.\n"
     ]
    }
   ],
   "source": [
    "print(f\"There are {df_collab['institution_id'].nunique():,} institutions that collaborate with UW, in {df_collab['institution_country_code'].nunique()} countries.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query the API for institutions data\n",
    "\n",
    "We've found about 20K institutions that collaborate with UW. We already know the names and the countries of these institutions; these were included in the results from the `works` endpoint of the API. There is some more information about the institutions that would be helpful to supply to the dashboard, especially the geolocation data that can be used to show the collaborations on maps. This information can be found in the [Institutions](https://docs.openalex.org/api-entities/institutions) entities.\n",
    "\n",
    "To get the institutions, we will query the `/institutions` endpoint of the API. We will use the technique to get multiple entities per API request as suggested [here](https://docs.openalex.org/how-to-use-the-api/get-lists-of-entities/filter-entity-lists#addition-or). This technique involves getting 50 results at once by requesting them in a filter, separated by a pipe ('`|`').\n",
    "\n",
    "This will get all of the institutions in about 400 API calls, which should only take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collected 20852 institutions using 418 api calls\n"
     ]
    }
   ],
   "source": [
    "institution_ids = df['institution_id'].dropna().unique()\n",
    "\n",
    "endpoint = \"institutions\"\n",
    "size = 50\n",
    "loop_index = 0\n",
    "institutions = []\n",
    "for list_index in range(0, len(institution_ids), size):\n",
    "    subset = institution_ids[list_index:list_index+size]\n",
    "    pipe_separated_ids = \"|\".join(subset)\n",
    "    r = requests.get(f\"https://api.openalex.org/institutions?filter=openalex:{pipe_separated_ids}&per-page={size}\")\n",
    "    # ! ids.openalex filter does not work, even though it is in the documentation\n",
    "    results = r.json()['results']\n",
    "    institutions.extend(results)\n",
    "    loop_index += 1\n",
    "print(f\"collected {len(institutions)} institutions using {loop_index} api calls\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, let's put this data into a Pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for institution in institutions:\n",
    "    data.append({\n",
    "        'id': institution['id'],\n",
    "        'ror': institution['ror'],\n",
    "        'display_name': institution['display_name'],\n",
    "        'country_code': institution['country_code'],\n",
    "        'type': institution['type'],\n",
    "        'latitude': institution['geo']['latitude'],\n",
    "        'longitude': institution['geo']['longitude'],\n",
    "        'city': institution['geo']['city'],\n",
    "        'region': institution['geo']['region'],\n",
    "        'country': institution['geo']['country'],\n",
    "        'image_url': institution['image_url'],\n",
    "        'image_thumbnail_url': institution['image_thumbnail_url'],\n",
    "    })\n",
    "\n",
    "df_institutions = pd.DataFrame(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the data\n",
    "\n",
    "Now it's time to save the data, so it can be handed off to the team at UW and be imported into their dashboard. We'll save two different files:\n",
    "- The publications data will have one row for each publication-author-affiliation triplet. Each publication will have multiple rows, because there are multiple authors per paper. An author in a given publication could have more than one row, if they have more than one affiliation.\n",
    "- The institutions data will have one row per institution, connected to the publications data by the `institution_id` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.isdir('../../data'):\n",
    "    os.mkdir('../../data')\n",
    "    \n",
    "# Save the publications data\n",
    "# Each row represents a publication-author-affiliation\n",
    "#\n",
    "# Compressing the data as a GZIP file will greatly reduce the file size.\n",
    "# Pandas knows to use GZIP compression if the file name ends in `.gz`\n",
    "outpath = '../../data/uw_collabs.csv.gz'\n",
    "df_collab.to_csv(outpath, index=False)\n",
    "\n",
    "# Save the institutions data\n",
    "outpath = '../../data/uw_collabs_institutions.csv.gz'\n",
    "df_institutions.to_csv(outpath, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "271691dbc4cdb85f541c883090ff5a004cbd8b9c207c2cfed84437fce4e65fdb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
